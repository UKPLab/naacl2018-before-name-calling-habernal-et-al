{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Verification of AH labels\n",
    "* How trustworthy are they?\n",
    "* Re-label some of AH comments (MTurk), also sample some negative instances\n",
    "\n",
    "## Negative instances sampling criteria\n",
    "* Sampled distribution must be the same in term of length (mean + std. dev.)\n",
    "* Sample delta comments (= these are high quality and manually verified)\n",
    "* Sample other removed comments but different label:\n",
    "\t* 5: 'Low effort post'\n",
    " \t* 1: 'Direct responses must challenge OP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive AH instances 3621\nNegative AH instances 22710\n"
     ]
    }
   ],
   "source": [
    "from AnnotatedRedditComment import AnnotatedRedditComment\n",
    "from RedditThread import RedditThread\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import pickle\n",
    "\n",
    "# First sample all instances and store them to pickle\n",
    "pickle_file = \"ah-positive-negative-instances-all.pkl\"\n",
    "\n",
    "\n",
    "def sample_validation_instances():\n",
    "    main_dir = '/home/user-ukp/data2/cmv-full-2017-09-22/'\n",
    "    files = [f for f in os.listdir(main_dir) if os.path.isfile(os.path.join(main_dir, f))]\n",
    "    \n",
    "    _positive_instances = set()\n",
    "    _negative_instances = set()\n",
    "    \n",
    "    for f in files:\n",
    "        comments = RedditThread.load_comments_from_file(os.path.join(main_dir, f))\n",
    "        clean_threads = RedditThread.discard_corrupted_threads(RedditThread.reconstruct_threads_from_submission(comments))\n",
    "        \n",
    "        for clean_thread in clean_threads:\n",
    "            assert isinstance(clean_thread, RedditThread)\n",
    "\n",
    "            has_deleted_comments = clean_thread.has_deleted_comments()\n",
    "            assert not has_deleted_comments, [\"(%d) '%s'\" % (len(comment.body), comment.body) for comment in clean_thread.comments]\n",
    "\n",
    "        # remove outliers (threads longer than 200 comments)\n",
    "        clean_threads = [thread for thread in clean_threads if 200 >= len(thread.comments) > 0]\n",
    "    \n",
    "        for comment in RedditThread.collect_all_comments(clean_threads):\n",
    "            # let's ignore extremely short and extremely long comments\n",
    "            if 20 < len(comment.body.strip()) < 2000:\n",
    "                assert isinstance(comment, AnnotatedRedditComment)\n",
    "                assert len(comment.body.strip()) > 0\n",
    "                \n",
    "                label = comment.violated_rule\n",
    "        \n",
    "                if label in (1, 5) or comment.delta:\n",
    "                    _negative_instances.add(comment)\n",
    "                elif label == 2:\n",
    "                    _positive_instances.add(comment)\n",
    "    \n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump((_positive_instances, _negative_instances), f)\n",
    "        f.close()\n",
    "        \n",
    "    # sort them as lists to ensure the non-random ordering if they were sets\n",
    "    return sorted(_positive_instances), sorted(_negative_instances)\n",
    "        \n",
    "\n",
    "# could be sets but sets cannot be ordered in Python\n",
    "positive_instances = list()\n",
    "negative_instances = list()\n",
    "\n",
    "if os.path.isfile(pickle_file):\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        positive_instances, negative_instances = pickle.load(f)\n",
    "else:\n",
    "    positive_instances, negative_instances = sample_validation_instances()\n",
    "    \n",
    "positive_instances = list(positive_instances)\n",
    "negative_instances = list(negative_instances)\n",
    "    \n",
    "print(\"Positive AH instances\", len(positive_instances))\n",
    "print(\"Negative AH instances\", len(negative_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now look at the length distributions\n",
    "\t* These are exponential, so let's estimate its parameters for future sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.stats\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn\n",
    "# \n",
    "# positive_instances_lengths = [len(_.body) for _ in positive_instances]\n",
    "# negative_instances_lengths = [len(_.body) for _ in negative_instances]\n",
    "# \n",
    "# print(\"Positive instances length stats:\", scipy.stats.describe(positive_instances_lengths))\n",
    "# print(\"Negative instances length stats:\", scipy.stats.describe(negative_instances_lengths))\n",
    "# \n",
    "# # estimate parameters of exponential distribution \n",
    "# loc_positive, scale_positive = scipy.stats.expon.fit(positive_instances_lengths)\n",
    "# \n",
    "# generated = scipy.stats.expon.rvs(loc_positive, scale_positive, len(positive_instances_lengths))\n",
    "# print(scipy.stats.describe(generated))\n",
    "# \n",
    "# # print the original and generated distribution... these are not same but close enough\n",
    "# seaborn.set(color_codes=True)\n",
    "# seaborn.distplot(positive_instances_lengths, kde=False)\n",
    "# seaborn.distplot(generated, kde=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now let's sample from both positive and negative sets, in each step a sample with the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605 keys from file distance_dict_0.pkl\nTotal size now is 605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605 keys from file distance_dict_1.pkl\nTotal size now is 1210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605 keys from file distance_dict_2.pkl\nTotal size now is 1815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605 keys from file distance_dict_3.pkl\nTotal size now is 2420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605 keys from file distance_dict_4.pkl\nTotal size now is 3025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 596 keys from file distance_dict_5.pkl\nTotal size now is 3621\n3621\nNegative instances 22710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3621\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "\n",
    "positive_to_negative_distances = dict()\n",
    "\n",
    "# read only first 6 files due to their extreme size\n",
    "for i in range(0, 6):\n",
    "    file_name = \"distance_dict_%d.pkl\" % i\n",
    "\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        current_dict = pickle.load(f)\n",
    "        assert isinstance(current_dict, dict)\n",
    "        print(\"Loaded %d keys from file %s\" % (len(current_dict), file_name))\n",
    "        f.close()\n",
    "        # add to the final\n",
    "        positive_to_negative_distances.update(current_dict)\n",
    "        print(\"Total size now is %d\" % len(positive_to_negative_distances))\n",
    "\n",
    "# load the serialized instance matrix\n",
    "# with open('distance_dict.pkl', 'rb') as f:\n",
    "#     positive_to_negative_distances = pickle.load(f)\n",
    "print(len(positive_to_negative_distances))\n",
    "\n",
    "# get a set of all negative instances so we can discard them after draw\n",
    "first_key = next(iter(positive_to_negative_distances))\n",
    "negative_instances_ids = set(positive_to_negative_distances[first_key].keys())\n",
    "\n",
    "print(\"Negative instances\", len(negative_instances_ids))\n",
    "\n",
    "# random sampling = 2,400 pairs in total\n",
    "# random_positive_instance_ids = random.sample(list(positive_to_negative_distances), 2400)\n",
    "# keep all of them now!\n",
    "random_positive_instance_ids = positive_to_negative_distances\n",
    "\n",
    "# print(random_positive_instance_ids)\n",
    "\n",
    "# list of tuples (positive_instance_id, negative_instance_id)\n",
    "samples = []\n",
    "\n",
    "for positive_instance_id in random_positive_instance_ids:\n",
    "    distances = positive_to_negative_distances[positive_instance_id]\n",
    "    assert isinstance(distances, dict)\n",
    "    assert all([isinstance(_, float) for _ in distances.values()])\n",
    "    \n",
    "    # print(\"Original negative samples available\", len(distances))\n",
    "\n",
    "    # update the negative candidates - retain only those which has not yet been drawn\n",
    "    # find the closest negative instance\n",
    "    distances_updated = {key: float(distances[key]) for key in negative_instances_ids}\n",
    "    assert isinstance(distances_updated, dict)\n",
    "    assert all([isinstance(_, float) for _ in distances_updated.values()])\n",
    "    \n",
    "    # print(\"Updated negative samples available\", len(distances_updated))\n",
    "    # print(\"D\", distances)\n",
    "    # print(\"DU\", distances_updated)\n",
    "    \n",
    "    # negative_instance_id = min(distances, key=distances_updated.get)\n",
    "    negative_instance_id = min(distances_updated.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    # print(\"Negative instance ID:\", negative_instance_id)\n",
    "    # print(distances_updated[negative_instance_id])\n",
    "\n",
    "    # delete sampled item\n",
    "    negative_instances_ids.remove(negative_instance_id)\n",
    "    \n",
    "    # and add the new sampled tuple\n",
    "    samples.append((positive_instance_id, negative_instance_id))\n",
    "    \n",
    "print(len(samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have _n_ samples of pairs (positive_id, negative_id) -> let's export them to JSON\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7242\n"
     ]
    }
   ],
   "source": [
    "# we have still two variables: positive instances and negative instances, no need to read full CMV again\n",
    "assert isinstance(positive_instances[0], AnnotatedRedditComment)\n",
    "assert isinstance(negative_instances[0], AnnotatedRedditComment)\n",
    "\n",
    "# group by ID\n",
    "positive_instances_by_id = {_.name: _ for _ in positive_instances}\n",
    "negative_instances_by_id = {_.name: _ for _ in negative_instances}\n",
    "\n",
    "\n",
    "list_for_export = []\n",
    "\n",
    "for (positive_id, negative_id) in samples:\n",
    "    list_for_export.append(positive_instances_by_id[positive_id])\n",
    "    list_for_export.append(negative_instances_by_id[negative_id])\n",
    "\n",
    "\n",
    "print(len(list_for_export))\n",
    "\n",
    "# write them to JSON files\n",
    "with open(\"experimental-data/exported-3621-sampled-positive-negative-ah-no-context.json\", \"w\") as f:\n",
    "    for comment in list_for_export:\n",
    "        assert isinstance(comment, AnnotatedRedditComment)\n",
    "        f.write(comment.to_json_string())\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing \"underperforming recall\" of moderators\n",
    "\n",
    "* Our previous experiment showed that 5% of AHs were missing label (negative samples were identified as positive). The negative sampling took into account only arguments with some existing label (such as \"low-effort post\", \"delta-awarded\", or similar). In the following experiment, we want to estimate the amount of AHs among all arguments in CMV that have no manual label at all. Therefore, we sample another set of negative instances (arguments) without labels such that they are similar to the positive instances (AHs).\n",
    "\n",
    "* How to do it?\n",
    "\n",
    "\t* Sample 223 AHs\n",
    "\t* Sample 20000 non-labeled instances\n",
    "\t* Compute similarity matrix (as in the previous experiment)\n",
    "\t* Find the most similar negative instances, resulting into 222 candidates\n",
    "\t* Annotate the candidates by 6 MTurkers with 0.9 MACE threshold, resulting into 200 labeled instances\n",
    "\t* See whether some of them are AH (or not)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
