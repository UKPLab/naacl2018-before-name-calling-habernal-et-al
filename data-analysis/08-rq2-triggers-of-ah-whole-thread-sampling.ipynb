{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find threads with at least one AH\n",
    "* Different context size - the larget the context, the fewer instances (i.e. context 4 gives ~1000 instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All AH threads: 1698\nAll delta threads: 19533\n"
     ]
    }
   ],
   "source": [
    "from RedditThread import RedditThread\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def filter_duplicate_threads(threads: list) -> list:\n",
    "    by_last_comment = {t.get_last_comment_name(): t for t in threads}\n",
    "    return list(by_last_comment.values())\n",
    "\n",
    "\n",
    "# how long the history must be?\n",
    "context_size = 2\n",
    "# context 3 gives 1291 threads -> it's ok\n",
    "# context 2 gives 1698 threads -> but that's too small, isn't it?\n",
    "# context 1 gives 3559 threads -> but that's too short context (only last argument / or delta)\n",
    "\n",
    "\n",
    "def filter_only_two_person_threads(threads: list) -> list:\n",
    "    \"\"\"\n",
    "    Creates a new list containing only threads with max two authors \n",
    "    :param threads: list of RedditThread instances, unchanged\n",
    "    :return: list of RedditThread instances\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for thread in threads:\n",
    "        authors = set([comment.author_name for comment in thread.comments])\n",
    "        # print(\"Authors:\", len(authors))\n",
    "        if len(authors) <= 2:\n",
    "            result.append(thread)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def sample_ah_threads_instances():\n",
    "    main_dir = '/home/user-ukp/data2/cmv-full-2017-09-22/'\n",
    "    files = [f for f in os.listdir(main_dir) if os.path.isfile(os.path.join(main_dir, f))]\n",
    "\n",
    "    # now extract all AH threads\n",
    "    threads_with_ah = []\n",
    "    threads_with_delta = []\n",
    "\n",
    "    for f in files:\n",
    "        comments = RedditThread.load_comments_from_file(os.path.join(main_dir, f))\n",
    "        clean_threads = RedditThread.discard_corrupted_threads(\n",
    "            RedditThread.reconstruct_threads_from_submission(comments))\n",
    "\n",
    "        for clean_thread in clean_threads:\n",
    "            assert isinstance(clean_thread, RedditThread)\n",
    "\n",
    "            has_deleted_comments = clean_thread.has_deleted_comments()\n",
    "            assert not has_deleted_comments, [\"(%d) '%s'\" % (len(comment.body), comment.body) for comment in\n",
    "                                              clean_thread.comments]\n",
    "\n",
    "        # remove outliers (threads longer than 200 comments)\n",
    "        # clean_threads = [thread for thread in clean_threads if 200 >= len(thread.comments) > 0]\n",
    "\n",
    "        for thread in clean_threads:\n",
    "            assert isinstance(thread, RedditThread)\n",
    "            if thread.has_some_ad_hominem():\n",
    "                first_ah_index = thread.get_positions_of_ad_hominem_comments()[0]\n",
    "\n",
    "                if first_ah_index >= context_size:\n",
    "                    # extract comment history and create a new thread\n",
    "                    shortened_thread = RedditThread()\n",
    "                    shortened_thread.comments = thread.comments[first_ah_index - context_size:first_ah_index + 1]\n",
    "\n",
    "                    threads_with_ah.append(shortened_thread)\n",
    "            elif thread.has_some_delta():\n",
    "                first_delta_index = thread.get_positions_of_delta_comments()[0]\n",
    "\n",
    "                # the delta comments is the fourth one here\n",
    "                if first_delta_index >= (context_size - 1):\n",
    "                    # extract comment history and create a new thread\n",
    "                    shortened_thread = RedditThread()\n",
    "                    shortened_thread.comments = thread.comments[\n",
    "                                                first_delta_index - (context_size - 1):first_delta_index + 1]\n",
    "\n",
    "                    threads_with_delta.append(shortened_thread)\n",
    "\n",
    "    # there are duplicates!!! - filter by the last comment\n",
    "    threads_with_ah = filter_only_two_person_threads(filter_duplicate_threads(threads_with_ah))\n",
    "    threads_with_delta = filter_only_two_person_threads(filter_duplicate_threads(threads_with_delta))\n",
    "\n",
    "    return threads_with_ah, threads_with_delta\n",
    "\n",
    "\n",
    "pickle_file = \"threads-with-ah-threads-with-delta-context%d.pkl\" % context_size\n",
    "if os.path.exists(pickle_file):\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        threads_with_ah, threads_with_delta = pickle.load(f)\n",
    "        f.close()\n",
    "else:\n",
    "    threads_with_ah, threads_with_delta = sample_ah_threads_instances()\n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump((threads_with_ah, threads_with_delta), f)\n",
    "        f.close()\n",
    "\n",
    "print(\"All AH threads:\", len(threads_with_ah))\n",
    "print(\"All delta threads:\", len(threads_with_delta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we must compute semantic similarity using external script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 425 keys from file threads-with-ah-threads-with-delta-context2-distances_0.pkl\nTotal size now is 425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 425 keys from file threads-with-ah-threads-with-delta-context2-distances_1.pkl\nTotal size now is 850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 425 keys from file threads-with-ah-threads-with-delta-context2-distances_2.pkl\nTotal size now is 1275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 423 keys from file threads-with-ah-threads-with-delta-context2-distances_3.pkl\nTotal size now is 1698\n1698\nNegative instances 19533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1698\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "\n",
    "positive_to_negative_distances = dict()\n",
    "\n",
    "for i in range(0, 4):\n",
    "    file_name = \"threads-with-ah-threads-with-delta-context%d-distances_%d.pkl\" % (context_size, i)\n",
    "\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        current_dict = pickle.load(f)\n",
    "        assert isinstance(current_dict, dict)\n",
    "        print(\"Loaded %d keys from file %s\" % (len(current_dict), file_name))\n",
    "        f.close()\n",
    "        # add to the final\n",
    "        positive_to_negative_distances.update(current_dict)\n",
    "        print(\"Total size now is %d\" % len(positive_to_negative_distances))\n",
    "\n",
    "print(len(positive_to_negative_distances))\n",
    "\n",
    "# get a set of all negative instances so we can discard them after draw\n",
    "first_key = next(iter(positive_to_negative_distances))\n",
    "negative_instances_ids = set(positive_to_negative_distances[first_key].keys())\n",
    "\n",
    "print(\"Negative instances\", len(negative_instances_ids))\n",
    "\n",
    "# a list of what? tuples (positive_id, negative_id)\n",
    "samples = []\n",
    "\n",
    "for positive_instance_id in positive_to_negative_distances:\n",
    "    distances = positive_to_negative_distances[positive_instance_id]\n",
    "    assert isinstance(distances, dict)\n",
    "    assert all([isinstance(_, float) for _ in distances.values()])\n",
    "    \n",
    "    # print(\"Original negative samples available\", len(distances))\n",
    "\n",
    "    # update the negative candidates - retain only those which has not yet been drawn\n",
    "    # find the closest negative instance\n",
    "    distances_updated = {key: float(distances[key]) for key in negative_instances_ids}\n",
    "    assert isinstance(distances_updated, dict)\n",
    "    assert all([isinstance(_, float) for _ in distances_updated.values()])\n",
    "    \n",
    "    # print(\"Updated negative samples available\", len(distances_updated))\n",
    "    # print(\"D\", distances)\n",
    "    # print(\"DU\", distances_updated)\n",
    "    \n",
    "    # negative_instance_id = min(distances, key=distances_updated.get)\n",
    "    negative_instance_id = min(distances_updated.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    # print(\"Negative instance ID:\", negative_instance_id)\n",
    "    # print(distances_updated[negative_instance_id])\n",
    "\n",
    "    # delete sampled item\n",
    "    negative_instances_ids.remove(negative_instance_id)\n",
    "    \n",
    "    # and add the new sampled tuple\n",
    "    samples.append((positive_instance_id, negative_instance_id))\n",
    "    \n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sample the threads and save to files\n",
    "* Each thread in the separate JSON file consisting of comments, each comment on one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from AnnotatedRedditComment import AnnotatedRedditComment\n",
    "\n",
    "output_folder = \"/tmp/sampled-threads-ah-delta-context%d\" % context_size\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "assert isinstance(samples[0], tuple)\n",
    "assert isinstance(threads_with_ah[0], RedditThread)\n",
    "assert isinstance(threads_with_delta[0], RedditThread)\n",
    "\n",
    "threads_ah_by_id = {t.get_last_comment_name(): t for t in threads_with_ah}\n",
    "threads_delta_by_id = {t.get_last_comment_name(): t for t in threads_with_delta}\n",
    "\n",
    "for counter, ah_id_delta_id in enumerate(samples):\n",
    "    ah_id, delta_id = ah_id_delta_id\n",
    "    # these are the two similar instances\n",
    "    thread_ah = threads_ah_by_id[ah_id]\n",
    "    thread_delta = threads_delta_by_id[delta_id]\n",
    "    \n",
    "    with open(os.path.join(output_folder, \"%d_ah_%s.json\" % (counter, ah_id)), \"w\") as f:\n",
    "        for c in thread_ah.comments:\n",
    "            assert isinstance(c, AnnotatedRedditComment)\n",
    "            f.write(c.to_json_string() + \"\\n\")\n",
    "        f.close()\n",
    "        \n",
    "    with open(os.path.join(output_folder, \"%d_delta_%s.json\" % (counter, delta_id)), \"w\") as f:\n",
    "        for c in thread_delta.comments:\n",
    "            assert isinstance(c, AnnotatedRedditComment)\n",
    "            f.write(c.to_json_string() + \"\\n\")\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
