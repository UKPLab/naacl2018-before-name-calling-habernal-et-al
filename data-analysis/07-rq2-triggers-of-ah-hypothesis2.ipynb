{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Triggers of AH - quantitive analysis\n",
    "\n",
    "Terminology: **AH(-1)** is a comment preceding AH (\"causing\"); **Delta** is a delta-awarded comment\n",
    "\n",
    "* We sample all AH(-1) and their most similar Delta comments\n",
    "* As in previous sampling, we filter out\n",
    "\t* extremely short and extremely long comments\n",
    "\t* threads with deleted comments\n",
    "\t* threads longer than 200 comments\n",
    "\t* AH(-1) cannot be the original post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AH(-1) instances 2994\nDelta instances 16559\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from AnnotatedRedditComment import AnnotatedRedditComment\n",
    "from RedditThread import RedditThread\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# First sample all instances and store them to pickle\n",
    "pickle_file = \"ah-minus1-delta-instances-all.pkl\"\n",
    "\n",
    "\n",
    "def sample_validation_instances():\n",
    "    main_dir = '/home/user-ukp/data2/cmv-full-2017-09-22/'\n",
    "    files = [f for f in os.listdir(main_dir) if os.path.isfile(os.path.join(main_dir, f))]\n",
    "    \n",
    "    _ah_minus1_instances = set()\n",
    "    _delta_instances = set()\n",
    "    \n",
    "    for f in files:\n",
    "        comments = RedditThread.load_comments_from_file(os.path.join(main_dir, f))\n",
    "        clean_threads = RedditThread.discard_corrupted_threads(RedditThread.reconstruct_threads_from_submission(comments))\n",
    "        \n",
    "        for clean_thread in clean_threads:\n",
    "            assert isinstance(clean_thread, RedditThread)\n",
    "\n",
    "            has_deleted_comments = clean_thread.has_deleted_comments()\n",
    "            assert not has_deleted_comments, [\"(%d) '%s'\" % (len(comment.body), comment.body) for comment in clean_thread.comments]\n",
    "\n",
    "        # remove outliers (threads longer than 200 comments)\n",
    "        clean_threads = [thread for thread in clean_threads if 200 >= len(thread.comments) > 0]\n",
    "    \n",
    "        # comments by ids\n",
    "        comments_by_id = {comment.name: comment for comment in RedditThread.collect_all_comments(clean_threads)\n",
    "                          if 20 < len(comment.body.strip()) < 2000}\n",
    "    \n",
    "        for comment_id in comments_by_id:\n",
    "            comment = comments_by_id[comment_id]\n",
    "            # let's ignore extremely short and extremely long comments\n",
    "            assert isinstance(comment, AnnotatedRedditComment)\n",
    "            assert len(comment.body.strip()) > 0\n",
    "            \n",
    "            label = comment.violated_rule\n",
    "    \n",
    "            if comment.delta:\n",
    "                _delta_instances.add(comment)\n",
    "            elif label == 2:\n",
    "                # get previous comment (only if not filtered out)\n",
    "                if comment.parent_id in comments_by_id:\n",
    "                    parent_comment = comments_by_id[comment.parent_id]\n",
    "                    assert isinstance(parent_comment, AnnotatedRedditComment)\n",
    "                    \n",
    "                    # and only if the parent is not OP (to avoid first-level AHs)\n",
    "                    if parent_comment.parent_id:\n",
    "                        _ah_minus1_instances.add(comment)\n",
    "\n",
    "    # convert to lists and shuffle\n",
    "    _ah_minus1_instances = list(_ah_minus1_instances)\n",
    "    _delta_instances = list(_delta_instances)\n",
    "    random.shuffle(_ah_minus1_instances)\n",
    "    random.shuffle(_delta_instances)\n",
    "\n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump((_ah_minus1_instances, _delta_instances), f)\n",
    "        f.close()\n",
    "        \n",
    "    return _ah_minus1_instances, _delta_instances\n",
    "        \n",
    "\n",
    "# use cache if possible\n",
    "if os.path.isfile(pickle_file):\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        ah_minus1_instances, delta_instances = pickle.load(f)\n",
    "else:\n",
    "    ah_minus1_instances, delta_instances = sample_validation_instances()\n",
    "    \n",
    "    \n",
    "print(\"AH(-1) instances\", len(ah_minus1_instances))\n",
    "print(\"Delta instances\", len(delta_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have to run the parallelized script on the cluster to compute all distances\n",
    "\n",
    "`python3 semantic-similarity-computation-parallel-step1.py` (modify the main method)\n",
    "\n",
    "* It will produce 5 files with distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605 keys from file ah-minus1-delta-distances_0.pkl\nTotal size now is 605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605 keys from file ah-minus1-delta-distances_1.pkl\nTotal size now is 1210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605 keys from file ah-minus1-delta-distances_2.pkl\nTotal size now is 1815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 605 keys from file ah-minus1-delta-distances_3.pkl\nTotal size now is 2420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 574 keys from file ah-minus1-delta-distances_4.pkl\nTotal size now is 2994\n2994\nNegative instances "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16559\n2994\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "\n",
    "positive_to_negative_distances = dict()\n",
    "\n",
    "for i in range(0, 5):\n",
    "    file_name = \"ah-minus1-delta-distances_%d.pkl\" % i\n",
    "\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        current_dict = pickle.load(f)\n",
    "        assert isinstance(current_dict, dict)\n",
    "        print(\"Loaded %d keys from file %s\" % (len(current_dict), file_name))\n",
    "        f.close()\n",
    "        # add to the final\n",
    "        positive_to_negative_distances.update(current_dict)\n",
    "        print(\"Total size now is %d\" % len(positive_to_negative_distances))\n",
    "\n",
    "print(len(positive_to_negative_distances))\n",
    "\n",
    "# get a set of all negative instances so we can discard them after draw\n",
    "first_key = next(iter(positive_to_negative_distances))\n",
    "negative_instances_ids = set(positive_to_negative_distances[first_key].keys())\n",
    "\n",
    "print(\"Negative instances\", len(negative_instances_ids))\n",
    "\n",
    "# random sampling = 2,400 pairs in total\n",
    "# random_positive_instance_ids = random.sample(list(positive_to_negative_distances), 2400)\n",
    "# keep all of them now!\n",
    "random_positive_instance_ids = positive_to_negative_distances\n",
    "\n",
    "# print(random_positive_instance_ids)\n",
    "\n",
    "# list of tuples (positive_instance_id, negative_instance_id)\n",
    "samples = []\n",
    "\n",
    "for positive_instance_id in random_positive_instance_ids:\n",
    "    distances = positive_to_negative_distances[positive_instance_id]\n",
    "    assert isinstance(distances, dict)\n",
    "    assert all([isinstance(_, float) for _ in distances.values()])\n",
    "    \n",
    "    # print(\"Original negative samples available\", len(distances))\n",
    "\n",
    "    # update the negative candidates - retain only those which has not yet been drawn\n",
    "    # find the closest negative instance\n",
    "    distances_updated = {key: float(distances[key]) for key in negative_instances_ids}\n",
    "    assert isinstance(distances_updated, dict)\n",
    "    assert all([isinstance(_, float) for _ in distances_updated.values()])\n",
    "    \n",
    "    # print(\"Updated negative samples available\", len(distances_updated))\n",
    "    # print(\"D\", distances)\n",
    "    # print(\"DU\", distances_updated)\n",
    "    \n",
    "    # negative_instance_id = min(distances, key=distances_updated.get)\n",
    "    negative_instance_id = min(distances_updated.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    # print(\"Negative instance ID:\", negative_instance_id)\n",
    "    # print(distances_updated[negative_instance_id])\n",
    "\n",
    "    # delete sampled item\n",
    "    negative_instances_ids.remove(negative_instance_id)\n",
    "    \n",
    "    # and add the new sampled tuple\n",
    "    samples.append((positive_instance_id, negative_instance_id))\n",
    "    \n",
    "print(len(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5988\n"
     ]
    }
   ],
   "source": [
    "# we have still two variables: positive instances and negative instances, no need to read full CMV again\n",
    "assert isinstance(ah_minus1_instances[0], AnnotatedRedditComment)\n",
    "assert isinstance(delta_instances[0], AnnotatedRedditComment)\n",
    "\n",
    "# group by ID\n",
    "positive_instances_by_id = {_.name: _ for _ in ah_minus1_instances}\n",
    "negative_instances_by_id = {_.name: _ for _ in delta_instances}\n",
    "\n",
    "\n",
    "list_for_export = []\n",
    "\n",
    "for (positive_id, negative_id) in samples:\n",
    "    list_for_export.append(positive_instances_by_id[positive_id])\n",
    "    list_for_export.append(negative_instances_by_id[negative_id])\n",
    "\n",
    "\n",
    "print(len(list_for_export))\n",
    "\n",
    "# write them to JSON files\n",
    "with open(\"experimental-data/exported-2994-sampled-pairs-ah-minus1-and-delta-context.json\", \"w\") as f:\n",
    "    for comment in list_for_export:\n",
    "        assert isinstance(comment, AnnotatedRedditComment)\n",
    "        f.write(comment.to_json_string())\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
