{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Properties of the OP comments\n",
    "\n",
    "* Investigate two properties: *controversy* and *stupidity*\n",
    "* Both likert scale (3)\n",
    "\n",
    "[Not controversial -- Somehow controversial -- Very controversial]\n",
    "\n",
    "Is this claim/topic reasonable to discuss or is it just stupid/silly?\n",
    "\n",
    "[Quite stupid -- Neutral -- Quite reasonable]\n",
    "\n",
    "Examples: \"Harry Potter is sexier than Spiderman\", \"Burritos are better than sandwiches\" -- these are quite stupid\n",
    "\"Nations whose leadership is based upon religion are fundamentally backwards\" -- quite reasonable topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First pilot\n",
    "\n",
    "* Sample some hundred OPs (among all OPs, not only AH threads) for pilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random 100 OP's from reddit\n",
    "from AnnotatedRedditComment import AnnotatedRedditComment\n",
    "from RedditThread import RedditThread\n",
    "import os\n",
    "import random\n",
    "\n",
    "# seed random generator\n",
    "random.seed(1234)\n",
    "\n",
    "\n",
    "def select_random_ops(_batch_indices: tuple) -> list:\n",
    "    \"\"\"\n",
    "    Samples a random list of original posts (OPs) from the complete CMV\n",
    "    :param _batch_indices: a tuple (from index, to index)\n",
    "    :return: list of a single AnnotatedRedditComment instances which are the OPs\n",
    "    \"\"\"\n",
    "    main_dir = '/home/user-ukp/data2/cmv-full-2017-09-22/'\n",
    "    all_files = [f for f in os.listdir(main_dir) if os.path.isfile(os.path.join(main_dir, f))]\n",
    "    random.shuffle(all_files)\n",
    "    files = all_files[_batch_indices[0]:_batch_indices[1]]\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for f in files:\n",
    "        comments = RedditThread.load_comments_from_file(os.path.join(main_dir, f))\n",
    "        clean_threads = RedditThread.discard_corrupted_threads(RedditThread.reconstruct_threads_from_submission(comments))\n",
    "        \n",
    "        if len(clean_threads):\n",
    "            _ = clean_threads[0].comments[0]\n",
    "            \n",
    "            assert isinstance(_, AnnotatedRedditComment)\n",
    "            result.append(_)\n",
    "        \n",
    "    return result\n",
    "    \n",
    "\n",
    "random_ops = select_random_ops((0, 100))\n",
    "# save to file\n",
    "with open(\"experimental-data/exported-100-random-ops.json\", \"w\") as f:\n",
    "    for op in random_ops:\n",
    "        assert isinstance(op, AnnotatedRedditComment)\n",
    "        f.write(op.to_json_string())\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "random_ops = select_random_ops((100, 1000))\n",
    "# save to file\n",
    "with open(\"experimental-data/exported-900-random-ops.json\", \"w\") as f:\n",
    "    for op in random_ops:\n",
    "        assert isinstance(op, AnnotatedRedditComment)\n",
    "        f.write(op.to_json_string())\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis of the first pilot, see `stupidity-controversy-pilot-plots.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling a full batch of 2,000 OPs\n",
    "\n",
    "Try to balance OPs: 1,000 OPs from AHs, 1,000 from those with delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n900\n"
     ]
    }
   ],
   "source": [
    "# seed random generator\n",
    "random.seed(1234)\n",
    "\n",
    "\n",
    "def select_random_ops_balanced() -> list:\n",
    "    \"\"\"\n",
    "    Samples a balanced random list of original posts (OPs) from the complete CMV: delta and AH submissions\n",
    "    :return: list of a single AnnotatedRedditComment instances which are the OPs\n",
    "    \"\"\"\n",
    "    main_dir = '/home/user-ukp/data2/cmv-full-2017-09-22/'\n",
    "    all_files = [f for f in os.listdir(main_dir) if os.path.isfile(os.path.join(main_dir, f))]\n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    max_samples_per_type = 900\n",
    "    \n",
    "    # OPs with AH or Delta\n",
    "    result_with_ah = []\n",
    "    result_with_delta = []\n",
    "    \n",
    "    for f in all_files:\n",
    "        if len(result_with_ah) >= max_samples_per_type and len(result_with_delta) >= max_samples_per_type:\n",
    "            break\n",
    "        \n",
    "        comments = RedditThread.load_comments_from_file(os.path.join(main_dir, f))\n",
    "        clean_threads = RedditThread.discard_corrupted_threads(RedditThread.reconstruct_threads_from_submission(comments))\n",
    "        \n",
    "        if len(clean_threads):\n",
    "            has_ad_hominem = any([_.has_some_ad_hominem() for _ in clean_threads])\n",
    "            has_delta = any([_.has_some_delta() for _ in clean_threads])\n",
    "            # print(\"Has AH/delta\", has_ad_hominem, has_delta, f)\n",
    "            \n",
    "            op = clean_threads[0].comments[0]\n",
    "            assert isinstance(op, AnnotatedRedditComment)\n",
    "            \n",
    "            # add only if buffer not full\n",
    "            if has_ad_hominem and not has_delta and len(result_with_ah) < max_samples_per_type:\n",
    "                result_with_ah.append(op)\n",
    "            elif has_delta and len(result_with_delta) < max_samples_per_type:\n",
    "                result_with_delta.append(op)\n",
    "            \n",
    "    print(len(result_with_ah))\n",
    "    print(len(result_with_delta))\n",
    "    \n",
    "    result = result_with_delta + result_with_ah\n",
    "    random.shuffle(result)\n",
    "    \n",
    "    return result \n",
    "        \n",
    "\n",
    "# export and save data to JSON\n",
    "sampled_balanced_ops = select_random_ops_balanced()\n",
    "\n",
    "with open(\"experimental-data/exported-1800-sampled-balanced-ops.json\", \"w\") as f:\n",
    "    for op in sampled_balanced_ops:\n",
    "        assert isinstance(op, AnnotatedRedditComment)\n",
    "        f.write(op.to_json_string())\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We found that there are only 967 submissions that have some AHs but no delta at all (aka \"very bad discussions\")\n",
    "\t* Therefore we sampled 900+900 OPs here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data for machine learning experiments\n",
    "\n",
    "* Let's export annotated data (1,800 instances) and all unlabeled OPs for machine learning experiments\n",
    "* The output format will be (per line):\n",
    "\t* `op_id  text  gold_label` for labeled data and `op_id  text` for unlabeled; tab-delimited\n",
    "\t* `gold_label` is a `double` value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from AnnotatedRedditComment import AnnotatedRedditComment\n",
    "from RedditThread import RedditThread\n",
    "\n",
    "\n",
    "def export_labeled_op(gold_json_file: str, output_tsv_gold: str, output_tsv_unlabeled: str)-> None:\n",
    "    # load gold annotations first\n",
    "    with open(gold_json_file) as f:\n",
    "        gold_labels = json.load(f)\n",
    "\n",
    "    main_dir = '/home/user-ukp/data2/cmv-full-2017-09-22/'\n",
    "    all_files = [f for f in os.listdir(main_dir) if os.path.isfile(os.path.join(main_dir, f))]\n",
    "\n",
    "    # open the output files    \n",
    "    with open(output_tsv_gold, 'w') as f_gold:\n",
    "        with open(output_tsv_unlabeled, 'w') as f_unlabeled:\n",
    "            \n",
    "            # read all threads\n",
    "            for f in all_files:\n",
    "                comments = RedditThread.load_comments_from_file(os.path.join(main_dir, f))\n",
    "                clean_threads = RedditThread.discard_corrupted_threads(\n",
    "                    RedditThread.reconstruct_threads_from_submission(comments))\n",
    "        \n",
    "                if len(clean_threads):\n",
    "                    op = clean_threads[0].comments[0]\n",
    "                    assert isinstance(op, AnnotatedRedditComment)\n",
    "        \n",
    "                    # now we have the OP -- let's find its gold label\n",
    "                    if op.name in gold_labels:\n",
    "                        # print(\"Gold label\", op.name, gold_labels[op.name])\n",
    "                        # and write to the gold file\n",
    "                        f_gold.write(\"%s\\t%s\\t%s\\n\" % (op.name, op.title, gold_labels[op.name]))\n",
    "                    else:\n",
    "                        # or as unlabeled\n",
    "                        f_unlabeled.write(\"%s\\t%s\\n\" % (op.name, op.title))\n",
    "            # don't forget to close the files\n",
    "            f_unlabeled.close()\n",
    "        f_gold.close()\n",
    "\n",
    "\n",
    "# export stupidity\n",
    "export_labeled_op('experimental-data/annotated-1800-sampled-balanced-ops-stupidity.json',\n",
    "                  'experimental-data/op-stupidity-controversy-prediction/stupidity-gold.tsv',\n",
    "                  'experimental-data/op-stupidity-controversy-prediction/stupidity-unlabeled.tsv')\n",
    "\n",
    "# export controversy\n",
    "export_labeled_op('experimental-data/annotated-1800-sampled-balanced-ops-controversy.json',\n",
    "                  'experimental-data/op-stupidity-controversy-prediction/controversy-gold.tsv',\n",
    "                  'experimental-data/op-stupidity-controversy-prediction/controversy-unlabeled.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
